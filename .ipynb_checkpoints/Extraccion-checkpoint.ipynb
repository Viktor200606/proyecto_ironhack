{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__autor__='Victor Garcia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install zipfile36\n",
    "#!{sys.executable} -m pip install pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "import urllib.request\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import pysrt\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraccion\n",
    "\n",
    "def descargar(pagina):\n",
    "    try:\n",
    "        peticion = urllib.request.Request(pagina)  \n",
    "        html = urllib.request.urlopen(peticion).read()\n",
    "        print(\"[*] Descarga OK >>\", pagina)\n",
    "    except:\n",
    "        print('[!] Error descargando',pagina)\n",
    "        return None\n",
    "        \n",
    "    return html\n",
    "    \n",
    "def rastrearEnlaces(pagina,c):\n",
    "    buscaEnlaces = re.compile('<a[^>]+href=\"({}\\-\\d+\\.html)\"'.format(c), re.IGNORECASE)\n",
    "    cola = queue.Queue()\n",
    "    cola.put(pagina) \n",
    "    visitados = [pagina]\n",
    "    print(\"Buscando enlaces en\",pagina)\n",
    "    while (cola.qsize() > 0):\n",
    "        html = descargar(cola.get())\n",
    "        if html == None:\n",
    "            continue\n",
    "        enlaces = buscaEnlaces.findall(str(html))\n",
    "        for enlace in enlaces:\n",
    "            enlace = urljoin(pagina, str(enlace))\n",
    "            if(enlace not in visitados):\n",
    "                cola.put(enlace)\n",
    "                visitados.append(enlace)\n",
    "    return visitados\n",
    "\n",
    "def web_scraper(url):\n",
    "    for i in range(1,13):\n",
    "        temporada = rastrearEnlaces(url.format(i),'episode')\n",
    "        temporada = temporada[1:]\n",
    "\n",
    "        libreto=[]\n",
    "        for e in temporada:\n",
    "            print('\\n*******episodios*******\\n')\n",
    "            episodio = rastrearEnlaces(e,'/subtitle')\n",
    "\n",
    "            print('\\n******subtitulos********\\n')\n",
    "            subtitulo = rastrearEnlaces(episodio[1],'download')\n",
    "            libreto.append(subtitulo[1])\n",
    "\n",
    "        navegador = webdriver.Chrome()\n",
    "        libreto=set(libreto)\n",
    "        for i in libreto:\n",
    "            navegador.get(i)\n",
    "\n",
    "#Descomprime los archivos\n",
    "def descomprime():\n",
    "    lista_subtitulos=os.listdir('C:/Users/A/Downloads')[1:]#('./subtitulos_zip')\n",
    "    lista_subtitulos\n",
    "\n",
    "    for path in lista_subtitulos:\n",
    "        path=\"C:/Users/A/Downloads/\"+path\n",
    "        zf=zipfile.ZipFile(path, \"r\")\n",
    "        for i in zf.namelist():\n",
    "            zf.extract(i, path='./subtitulos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformacion\n",
    "\n",
    "def clean():\n",
    "    lista_subtitulos=os.listdir('./subtitulos')\n",
    "\n",
    "    for path in lista_subtitulos:\n",
    "        try:\n",
    "            path=\"./subtitulos/\"+path\n",
    "            subs = pysrt.open(path)\n",
    "            archivo = open(\"chatbot.txt\", \"a+\")\n",
    "\n",
    "            for i in range(len(subs)):\n",
    "                k=subs[i].text.replace('<i>', '').replace('</i>', ' ').replace('\\n', ' ').replace('  ', ' ').replace('.', '')\\\n",
    "                         .replace('?','').replace('!','').replace('(','').replace(')','').replace('\"','').replace('o/~','')\n",
    "                k+='. '\n",
    "                try:\n",
    "                    print(k, file=archivo)\n",
    "                except:\n",
    "                    pass\n",
    "            archivo.close()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=[\"http://es.tvsubtitles.net/tvshow-154-{}.html\", 'http://es.tvsubtitles.net/tvshow-1810-{}.html']\n",
    "for u in url:\n",
    "    web_scraper(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "descomprime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
